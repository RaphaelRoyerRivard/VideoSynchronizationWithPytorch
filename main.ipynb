{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn To Synchronize Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import sync_net\n",
    "import trainer\n",
    "import metrics\n",
    "import data_loader\n",
    "reload(sync_net)\n",
    "reload(trainer)\n",
    "reload(metrics)\n",
    "reload(data_loader)\n",
    "from sync_net import reset_first_layer, replace_last_layer, add_sigmoid_activation, TripletNet, MultiSiameseNet, TripletLoss, CosineSimilarityTripletLoss, LosslessTripletLoss, MultiSiameseCosineSimilarityLoss\n",
    "from data_loader import get_datasets, get_test_set, get_multisiamese_datasets\n",
    "from trainer import fit\n",
    "from metrics import EmbeddingL2DistanceMetric, EmbeddingCosineSimilarityMetric\n",
    "# import wandb\n",
    "# wandb.init(project=\"learn-to-synchronize-videos\")\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "embedding_net = models.resnet50(pretrained=True)\n",
    "reset_first_layer(embedding_net)\n",
    "replace_last_layer(embedding_net, 16)\n",
    "# embedding_net = add_sigmoid_activation(embedding_net)  # Use only with LosslessTripletLoss\n",
    "# model = TripletNet(embedding_net)\n",
    "model = MultiSiameseNet(embedding_net)\n",
    "model.cuda(0)\n",
    "model = nn.DataParallel(model).cuda()\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# loss_fn = TripletLoss(margin=0.5)\n",
    "# loss_fn = CosineSimilarityTripletLoss(margin=0.5)\n",
    "# loss_fn = LosslessTripletLoss()\n",
    "loss_fn = MultiSiameseCosineSimilarityLoss()\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 20\n",
    "log_interval = 100\n",
    "start_epoch = 0\n",
    "save_path = r\"C:\\Users\\root\\Projects\\VideoSynchronizationWithPytorch\\trainings\\angio_seq_multisiamese_fast\"\n",
    "if not os.path.isdir(save_path):\n",
    "    os.makedirs(save_path)\n",
    "# wandb.config.lr = lr\n",
    "# wandb.config.optimizer = type(optimizer)\n",
    "# wandb.config.loss_fn = type(loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Angio sequence triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = r'C:\\Users\\root\\Data\\Angiographie'\n",
    "validation_path = r'C:\\Users\\root\\Data\\Angiographie\\KR-11'\n",
    "training_set, validation_set = get_datasets(training_path, validation_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Angio sequence multi siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 valid frames in C:\\Users\\root\\Data\\Angiographie\\AA-4\\export\\LCA_30LAO25CAU\n",
      "110 valid frames in C:\\Users\\root\\Data\\Angiographie\\AA-4\\export\\LCA_30RAO\n",
      "104 valid frames in C:\\Users\\root\\Data\\Angiographie\\AA-4\\export\\LCA_30RAO25CAU\n",
      "78 valid frames in C:\\Users\\root\\Data\\Angiographie\\AA-4\\export\\LCA_AP\n",
      "79 valid frames in C:\\Users\\root\\Data\\Angiographie\\AA-4\\export\\LCA_LAT\n",
      "121 valid frames in C:\\Users\\root\\Data\\Angiographie\\AA-4\\export\\RCA_AP\n",
      "113 valid frames in C:\\Users\\root\\Data\\Angiographie\\AA-4\\export\\RCA_LAT\n",
      "75 valid frames in C:\\Users\\root\\Data\\Angiographie\\ABL-5\\export\\LCA_30LAO25CRA\n",
      "78 valid frames in C:\\Users\\root\\Data\\Angiographie\\ABL-5\\export\\LCA_30RAO\n",
      "85 valid frames in C:\\Users\\root\\Data\\Angiographie\\ABL-5\\export\\LCA_AP\n",
      "85 valid frames in C:\\Users\\root\\Data\\Angiographie\\ABL-5\\export\\LCA_LAT\n",
      "39 valid frames in C:\\Users\\root\\Data\\Angiographie\\ABL-5\\export\\RCA_AP\n",
      "39 valid frames in C:\\Users\\root\\Data\\Angiographie\\ABL-5\\export\\RCA_LAT\n",
      "48 valid frames in C:\\Users\\root\\Data\\Angiographie\\AC-1\\export\\LCA_40RA015CAU\n",
      "44 valid frames in C:\\Users\\root\\Data\\Angiographie\\AC-1\\export\\LCA_45LAO\n",
      "49 valid frames in C:\\Users\\root\\Data\\Angiographie\\AC-1\\export\\LCA_AP\n",
      "51 valid frames in C:\\Users\\root\\Data\\Angiographie\\AC-1\\export\\LCA_LAT\n",
      "54 valid frames in C:\\Users\\root\\Data\\Angiographie\\AC-1\\export\\RCA_AP\n",
      "60 valid frames in C:\\Users\\root\\Data\\Angiographie\\AC-1\\export\\RCA_LAT\n",
      "51 valid frames in C:\\Users\\root\\Data\\Angiographie\\ALR-2\\export\\LCA_30LAO25CRA\n",
      "46 valid frames in C:\\Users\\root\\Data\\Angiographie\\ALR-2\\export\\LCA_30RAO\n",
      "48 valid frames in C:\\Users\\root\\Data\\Angiographie\\ALR-2\\export\\LCA_AP\n",
      "49 valid frames in C:\\Users\\root\\Data\\Angiographie\\ALR-2\\export\\LCA_LAT\n",
      "62 valid frames in C:\\Users\\root\\Data\\Angiographie\\ALR-2\\export\\RCA_AP\n",
      "53 valid frames in C:\\Users\\root\\Data\\Angiographie\\JEL-10\\export\\LCA_30RA025CAU\n",
      "45 valid frames in C:\\Users\\root\\Data\\Angiographie\\JEL-10\\export\\LCA_30RAO\n",
      "31 valid frames in C:\\Users\\root\\Data\\Angiographie\\JEL-10\\export\\LCA_AP\n",
      "70 valid frames in C:\\Users\\root\\Data\\Angiographie\\JEL-10\\export\\LCA_LAT\n",
      "65 valid frames in C:\\Users\\root\\Data\\Angiographie\\JEL-10\\export\\RCA_AP\n",
      "44 valid frames in C:\\Users\\root\\Data\\Angiographie\\JEL-10\\export\\RCA_LAT\n",
      "84 valid frames in C:\\Users\\root\\Data\\Angiographie\\KC-3\\export\\LCA_30RAO\n",
      "99 valid frames in C:\\Users\\root\\Data\\Angiographie\\KC-3\\export\\LCA_30RAO25CAU\n",
      "82 valid frames in C:\\Users\\root\\Data\\Angiographie\\KC-3\\export\\LCA_AP\n",
      "75 valid frames in C:\\Users\\root\\Data\\Angiographie\\KC-3\\export\\LCA_LAT\n",
      "94 valid frames in C:\\Users\\root\\Data\\Angiographie\\KC-3\\export\\RCA_AP\n",
      "107 valid frames in C:\\Users\\root\\Data\\Angiographie\\KC-3\\export\\RCA_LAT\n",
      "50 valid frames in C:\\Users\\root\\Data\\Angiographie\\MAL-8\\export\\LCA_AP\n",
      "47 valid frames in C:\\Users\\root\\Data\\Angiographie\\MAL-8\\export\\LCA_LAT\n",
      "41 valid frames in C:\\Users\\root\\Data\\Angiographie\\MAL-8\\export\\RCA_AP\n",
      "41 valid frames in C:\\Users\\root\\Data\\Angiographie\\MAL-8\\export\\RCA_LAT\n",
      "52 valid frames in C:\\Users\\root\\Data\\Angiographie\\MB-12\\export\\LCA_30LAO25CRA\n",
      "55 valid frames in C:\\Users\\root\\Data\\Angiographie\\MB-12\\export\\LCA_30RAO\n",
      "51 valid frames in C:\\Users\\root\\Data\\Angiographie\\MB-12\\export\\LCA_30RAO25CAU\n",
      "51 valid frames in C:\\Users\\root\\Data\\Angiographie\\MB-12\\export\\LCA_AP\n",
      "52 valid frames in C:\\Users\\root\\Data\\Angiographie\\MB-12\\export\\LCA_LAT\n",
      "46 valid frames in C:\\Users\\root\\Data\\Angiographie\\MJY-9\\export\\RCA_AP\n",
      "52 valid frames in C:\\Users\\root\\Data\\Angiographie\\MJY-9\\export\\RCA_LAT\n",
      "43 valid frames in C:\\Users\\root\\Data\\Angiographie\\SB-6\\export\\RCA_AP\n",
      "41 valid frames in C:\\Users\\root\\Data\\Angiographie\\SB-6\\export\\RCA_LAT\n",
      "39 valid frames in C:\\Users\\root\\Data\\Angiographie\\KR-11\\export\\LCA_30LAO25CAU\n",
      "42 valid frames in C:\\Users\\root\\Data\\Angiographie\\KR-11\\export\\LCA_30LAO25CRA\n",
      "41 valid frames in C:\\Users\\root\\Data\\Angiographie\\KR-11\\export\\LCA_30RAO\n",
      "50 valid frames in C:\\Users\\root\\Data\\Angiographie\\KR-11\\export\\LCA_AP\n",
      "45 valid frames in C:\\Users\\root\\Data\\Angiographie\\KR-11\\export\\LCA_LAT\n",
      "63 valid frames in C:\\Users\\root\\Data\\Angiographie\\KR-11\\export\\RCA_AP\n",
      "76 valid frames in C:\\Users\\root\\Data\\Angiographie\\KR-11\\export\\RCA_LAT\n"
     ]
    }
   ],
   "source": [
    "training_path = r'C:\\Users\\root\\Data\\Angiographie'\n",
    "validation_path = r'C:\\Users\\root\\Data\\Angiographie\\KR-11'\n",
    "training_set, validation_set = get_multisiamese_datasets(training_path, validation_path, 1000, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST dataset instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transformations = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "mnist_trainset = datasets.MNIST(r\"C:\\Users\\root\\Data\\MNIST\", train=True, download=True, transform=transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TripletMNIST(Dataset):\n",
    "    \"\"\"\n",
    "    Train: For each sample (anchor) randomly chooses a positive and negative samples\n",
    "    Test: Creates fixed triplets for testing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mnist_dataset):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.train = self.mnist_dataset.train\n",
    "        self.transform = self.mnist_dataset.transform\n",
    "\n",
    "        if self.train:\n",
    "            self.train_labels = self.mnist_dataset.train_labels\n",
    "            self.train_data = self.mnist_dataset.train_data\n",
    "            self.labels_set = set(self.train_labels.numpy())\n",
    "            self.label_to_indices = {label: np.where(self.train_labels.numpy() == label)[0]\n",
    "                                     for label in self.labels_set}\n",
    "\n",
    "        else:\n",
    "            self.test_labels = self.mnist_dataset.test_labels\n",
    "            self.test_data = self.mnist_dataset.test_data\n",
    "            # generate fixed triplets for testing\n",
    "            self.labels_set = set(self.test_labels.numpy())\n",
    "            self.label_to_indices = {label: np.where(self.test_labels.numpy() == label)[0]\n",
    "                                     for label in self.labels_set}\n",
    "\n",
    "            random_state = np.random.RandomState(29)\n",
    "\n",
    "            triplets = [[i,\n",
    "                         random_state.choice(self.label_to_indices[self.test_labels[i].item()]),\n",
    "                         random_state.choice(self.label_to_indices[\n",
    "                                                 np.random.choice(\n",
    "                                                     list(self.labels_set - set([self.test_labels[i].item()]))\n",
    "                                                 )\n",
    "                                             ])\n",
    "                         ]\n",
    "                        for i in range(len(self.test_data))]\n",
    "            self.test_triplets = triplets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            img1, label1 = self.train_data[index], self.train_labels[index].item()\n",
    "            positive_index = index\n",
    "            while positive_index == index:\n",
    "                positive_index = np.random.choice(self.label_to_indices[label1])\n",
    "            negative_label = np.random.choice(list(self.labels_set - set([label1])))\n",
    "            negative_index = np.random.choice(self.label_to_indices[negative_label])\n",
    "            img2 = self.train_data[positive_index]\n",
    "            img3 = self.train_data[negative_index]\n",
    "        else:\n",
    "            img1 = self.test_data[self.test_triplets[index][0]]\n",
    "            img2 = self.test_data[self.test_triplets[index][1]]\n",
    "            img3 = self.test_data[self.test_triplets[index][2]]\n",
    "\n",
    "        img1 = Image.fromarray(img1.numpy(), mode='L')\n",
    "        img2 = Image.fromarray(img2.numpy(), mode='L')\n",
    "        img3 = Image.fromarray(img3.numpy(), mode='L')\n",
    "        if self.transform is not None:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "            img3 = self.transform(img3)\n",
    "        return (img1, img2, img3)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)\n",
    "    \n",
    "    \n",
    "triplet_mnist = TripletMNIST(mnist_trainset)\n",
    "train_loader = DataLoader(triplet_mnist, batch_size=20, shuffle=True, num_workers=0)\n",
    "# for batch_index, triplet in enumerate(train_loader):\n",
    "#     for i, image in enumerate(triplet):\n",
    "#         print(f\"batch {batch_index}, i {i}, triplet {image.numpy().shape}\")\n",
    "#         plt.subplot(1, 3, i+1)\n",
    "#         plt.imshow(image.view(224, 224).numpy())\n",
    "#         plt.title(\"Anchor\" if i == 0 else \"Positive\" if i == 1 else \"Negative\")\n",
    "#     plt.show()\n",
    "metrics = [EmbeddingL2DistanceMetric(), EmbeddingCosineSimilarityMetric()]\n",
    "fit(train_loader, None, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, start_epoch=start_epoch, save_progress_path=save_path, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_state_path = save_path + r\"\\training_state_0.pth\"\n",
    "print(load_state_path)\n",
    "state = torch.load(load_state_path)\n",
    "\n",
    "start_epoch = int(state['epoch']) + 1\n",
    "model.load_state_dict(state['model'])\n",
    "optimizer.load_state_dict(state['optimizer'])\n",
    "scheduler.load_state_dict(state['scheduler'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()  # Doesn't always work to free the GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Angio sequence triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(training_set, batch_size=20, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(validation_set, batch_size=20, shuffle=True, num_workers=4)\n",
    "metrics = [EmbeddingL2DistanceMetric(), EmbeddingCosineSimilarityMetric()]\n",
    "fit(train_loader, val_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, start_epoch=start_epoch, save_progress_path=save_path, metrics=metrics, measure_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Angio sequence multisiamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 0\n",
      "Will sample from train_loader\n",
      "Train: [0/1000 (0%)]\tLoss: 2.005047\tElapsed time: 0:00:02.910162\tFCWeights (Diff, Avg, Total): (32.724395751953125, 0.02693011239171028, 882.4459228515625)\n",
      "Train: [4700/1000 (10%)]\tLoss: 1.277247\tElapsed time: 0:00:45.254737\tFCWeights (Diff, Avg, Total): (7.641147613525391, 0.027733132243156433, 908.75927734375)\n",
      "Train: [9400/1000 (20%)]\tLoss: 1.196010\tElapsed time: 0:01:26.998018\tFCWeights (Diff, Avg, Total): (3.1830101013183594, 0.028224743902683258, 924.868408203125)\n",
      "Train: [19200/1000 (30%)]\tLoss: 1.139183\tElapsed time: 0:02:09.483646\tFCWeights (Diff, Avg, Total): (4.511167526245117, 0.028908621519804, 947.2777099609375)\n",
      "Train: [17600/1000 (40%)]\tLoss: 1.023762\tElapsed time: 0:02:50.713321\tFCWeights (Diff, Avg, Total): (5.990719795227051, 0.029112346470355988, 953.953369140625)\n",
      "Train: [32000/1000 (50%)]\tLoss: 1.103233\tElapsed time: 0:03:32.942116\tFCWeights (Diff, Avg, Total): (3.797576427459717, 0.029740609228610992, 974.540283203125)\n",
      "Train: [22200/1000 (60%)]\tLoss: 0.962304\tElapsed time: 0:04:15.373209\tFCWeights (Diff, Avg, Total): (1.7288130521774292, 0.030119065195322037, 986.9415283203125)\n",
      "Train: [44800/1000 (70%)]\tLoss: 1.072714\tElapsed time: 0:04:56.834963\tFCWeights (Diff, Avg, Total): (2.984922409057617, 0.03098401054739952, 1015.2840576171875)\n",
      "Train: [51200/1000 (80%)]\tLoss: 1.028270\tElapsed time: 0:05:40.949553\tFCWeights (Diff, Avg, Total): (1.8050273656845093, 0.03125445172190666, 1024.1458740234375)\n",
      "Train: [57600/1000 (90%)]\tLoss: 1.073461\tElapsed time: 0:06:23.416156\tFCWeights (Diff, Avg, Total): (1.8580472469329834, 0.03210540488362312, 1052.0299072265625)\n",
      "Epoch: 1/20. Train set: Average loss: 1.0880\tFCWeights (Diff, Avg, Total): (571.460693359375, 0.03263562172651291, 1069.404052734375)\n",
      "Starting Epoch 1\n",
      "Will sample from train_loader\n",
      "Train: [0/1000 (0%)]\tLoss: 0.619906\tElapsed time: 0:00:00.389996\tFCWeights (Diff, Avg, Total): (1.2895705699920654, 0.03263864293694496, 1069.5030517578125)\n",
      "Train: [6400/1000 (10%)]\tLoss: 1.024588\tElapsed time: 0:00:43.687549\tFCWeights (Diff, Avg, Total): (6.104750156402588, 0.032886408269405365, 1077.621826171875)\n",
      "Train: [9800/1000 (20%)]\tLoss: 0.975870\tElapsed time: 0:01:27.399343\tFCWeights (Diff, Avg, Total): (1.6604576110839844, 0.033291272819042206, 1090.888427734375)\n",
      "Train: [12900/1000 (30%)]\tLoss: 1.009182\tElapsed time: 0:02:09.363414\tFCWeights (Diff, Avg, Total): (2.5688812732696533, 0.034149304032325745, 1119.00439453125)\n",
      "Train: [23200/1000 (40%)]\tLoss: 1.034237\tElapsed time: 0:02:51.353382\tFCWeights (Diff, Avg, Total): (2.5658316612243652, 0.03460632637143135, 1133.9801025390625)\n",
      "Train: [32000/1000 (50%)]\tLoss: 0.931544\tElapsed time: 0:03:34.607852\tFCWeights (Diff, Avg, Total): (1.940499186515808, 0.03477327525615692, 1139.45068359375)\n",
      "Train: [25200/1000 (60%)]\tLoss: 0.971535\tElapsed time: 0:04:15.992123\tFCWeights (Diff, Avg, Total): (4.106678009033203, 0.035093583166599274, 1149.946533203125)\n",
      "Train: [35700/1000 (70%)]\tLoss: 0.983905\tElapsed time: 0:04:58.604530\tFCWeights (Diff, Avg, Total): (3.748941659927368, 0.03560662269592285, 1166.7578125)\n",
      "Train: [33600/1000 (80%)]\tLoss: 0.994993\tElapsed time: 0:05:41.201878\tFCWeights (Diff, Avg, Total): (5.486367225646973, 0.03603801876306534, 1180.893798828125)\n",
      "Train: [52200/1000 (90%)]\tLoss: 0.911037\tElapsed time: 0:06:22.590035\tFCWeights (Diff, Avg, Total): (6.518513202667236, 0.03627078980207443, 1188.521240234375)\n",
      "Epoch: 2/20. Train set: Average loss: 0.9766\tFCWeights (Diff, Avg, Total): (243.10118103027344, 0.036762505769729614, 1204.6337890625)\n",
      "Starting Epoch 2\n",
      "Will sample from train_loader\n",
      "Train: [0/1000 (0%)]\tLoss: 0.821844\tElapsed time: 0:00:00.497002\tFCWeights (Diff, Avg, Total): (2.808886766433716, 0.03675978258252144, 1204.5445556640625)\n",
      "Train: [6000/1000 (10%)]\tLoss: 0.971218\tElapsed time: 0:00:43.448813\tFCWeights (Diff, Avg, Total): (0.2547070384025574, 0.03689872846007347, 1209.0975341796875)\n",
      "Train: [12800/1000 (20%)]\tLoss: 0.981044\tElapsed time: 0:01:26.201627\tFCWeights (Diff, Avg, Total): (2.062854766845703, 0.037332162261009216, 1223.30029296875)\n",
      "Train: [12600/1000 (30%)]\tLoss: 0.904273\tElapsed time: 0:02:08.597331\tFCWeights (Diff, Avg, Total): (3.5529274940490723, 0.03741651028394699, 1226.064208984375)\n",
      "Train: [18400/1000 (40%)]\tLoss: 0.988829\tElapsed time: 0:02:52.646100\tFCWeights (Diff, Avg, Total): (8.926261901855469, 0.03773978725075722, 1236.6573486328125)\n",
      "Train: [32000/1000 (50%)]\tLoss: 0.992806\tElapsed time: 0:03:34.906815\tFCWeights (Diff, Avg, Total): (2.73230242729187, 0.03802888095378876, 1246.13037109375)\n",
      "Train: [23400/1000 (60%)]\tLoss: 1.180493\tElapsed time: 0:04:17.378927\tFCWeights (Diff, Avg, Total): (5.880692481994629, 0.038833796977996826, 1272.505859375)\n",
      "Train: [40600/1000 (70%)]\tLoss: 1.052949\tElapsed time: 0:05:00.538373\tFCWeights (Diff, Avg, Total): (3.613412618637085, 0.03934246301651001, 1289.173828125)\n",
      "Train: [40000/1000 (80%)]\tLoss: 0.995940\tElapsed time: 0:05:42.325110\tFCWeights (Diff, Avg, Total): (1.6214046478271484, 0.03981202095746994, 1304.560302734375)\n",
      "Train: [45000/1000 (90%)]\tLoss: 0.941400\tElapsed time: 0:06:23.968288\tFCWeights (Diff, Avg, Total): (6.807469844818115, 0.04024186357855797, 1318.6453857421875)\n",
      "Epoch: 3/20. Train set: Average loss: 0.9981\tFCWeights (Diff, Avg, Total): (199.15589904785156, 0.04068593680858612, 1333.19677734375)\n",
      "Starting Epoch 3\n",
      "Will sample from train_loader\n",
      "Train: [0/1000 (0%)]\tLoss: 1.902845\tElapsed time: 0:00:00.491100\tFCWeights (Diff, Avg, Total): (8.904356002807617, 0.04070679098367691, 1333.880126953125)\n",
      "Train: [6000/1000 (10%)]\tLoss: 1.061156\tElapsed time: 0:00:42.683774\tFCWeights (Diff, Avg, Total): (1.5965783596038818, 0.04157526418566704, 1362.3382568359375)\n",
      "Train: [7800/1000 (20%)]\tLoss: 0.956972\tElapsed time: 0:01:25.450604\tFCWeights (Diff, Avg, Total): (5.462084770202637, 0.04176924750208855, 1368.6947021484375)\n",
      "Train: [14100/1000 (30%)]\tLoss: 0.928326\tElapsed time: 0:02:07.985004\tFCWeights (Diff, Avg, Total): (6.194666862487793, 0.04268389567732811, 1398.6658935546875)\n",
      "Train: [17200/1000 (40%)]\tLoss: 0.895528\tElapsed time: 0:02:50.037095\tFCWeights (Diff, Avg, Total): (3.490838050842285, 0.04330968111753464, 1419.171630859375)\n",
      "Train: [31500/1000 (50%)]\tLoss: 0.932273\tElapsed time: 0:03:32.224744\tFCWeights (Diff, Avg, Total): (4.742894172668457, 0.043834663927555084, 1436.374267578125)\n",
      "Train: [30000/1000 (60%)]\tLoss: 0.919114\tElapsed time: 0:04:14.878906\tFCWeights (Diff, Avg, Total): (1.5609875917434692, 0.044280942529439926, 1450.9979248046875)\n",
      "Train: [44800/1000 (70%)]\tLoss: 0.960456\tElapsed time: 0:04:58.208440\tFCWeights (Diff, Avg, Total): (1.2247604131698608, 0.044417377561330795, 1455.4686279296875)\n",
      "Train: [39200/1000 (80%)]\tLoss: 0.927335\tElapsed time: 0:05:40.913557\tFCWeights (Diff, Avg, Total): (1.0171575546264648, 0.044513192027807236, 1458.6082763671875)\n",
      "Train: [35100/1000 (90%)]\tLoss: 0.872288\tElapsed time: 0:06:23.244967\tFCWeights (Diff, Avg, Total): (1.6424832344055176, 0.04466918110847473, 1463.7197265625)\n",
      "Epoch: 4/20. Train set: Average loss: 0.9393\tFCWeights (Diff, Avg, Total): (182.27713012695312, 0.0446598082780838, 1463.41259765625)\n",
      "Starting Epoch 4\n",
      "Will sample from train_loader\n",
      "Train: [0/1000 (0%)]\tLoss: 1.308155\tElapsed time: 0:00:00.477097\tFCWeights (Diff, Avg, Total): (2.4207875728607178, 0.044660381972789764, 1463.431396484375)\n",
      "Train: [4100/1000 (10%)]\tLoss: 0.880958\tElapsed time: 0:00:41.756784\tFCWeights (Diff, Avg, Total): (0.43730980157852173, 0.04539158195257187, 1487.391357421875)\n",
      "Train: [10200/1000 (20%)]\tLoss: 0.922975\tElapsed time: 0:01:24.075909\tFCWeights (Diff, Avg, Total): (2.8997802734375, 0.04554829001426697, 1492.5263671875)\n",
      "Train: [15300/1000 (30%)]\tLoss: 1.012642\tElapsed time: 0:02:07.615322\tFCWeights (Diff, Avg, Total): (2.060580015182495, 0.04557589814066887, 1493.4310302734375)\n",
      "Train: [11600/1000 (40%)]\tLoss: 0.992281\tElapsed time: 0:02:50.281021\tFCWeights (Diff, Avg, Total): (13.459259033203125, 0.04562804475426674, 1495.1397705078125)\n",
      "Train: [22000/1000 (50%)]\tLoss: 1.017371\tElapsed time: 0:03:31.721259\tFCWeights (Diff, Avg, Total): (1.117170810699463, 0.04687941074371338, 1536.14453125)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [23400/1000 (60%)]\tLoss: 1.009998\tElapsed time: 0:04:14.794509\tFCWeights (Diff, Avg, Total): (8.343379020690918, 0.048917025327682495, 1602.9130859375)\n",
      "Train: [44800/1000 (70%)]\tLoss: 0.916423\tElapsed time: 0:04:56.670220\tFCWeights (Diff, Avg, Total): (4.534055233001709, 0.0490608736872673, 1607.626708984375)\n",
      "Train: [41600/1000 (80%)]\tLoss: 0.944888\tElapsed time: 0:05:39.454592\tFCWeights (Diff, Avg, Total): (2.9678328037261963, 0.04936303198337555, 1617.52783203125)\n",
      "Train: [57600/1000 (90%)]\tLoss: 0.948088\tElapsed time: 0:06:20.555101\tFCWeights (Diff, Avg, Total): (6.124423027038574, 0.04986077547073364, 1633.837890625)\n",
      "Epoch: 5/20. Train set: Average loss: 0.9651\tFCWeights (Diff, Avg, Total): (234.11199951171875, 0.050286196172237396, 1647.778076171875)\n",
      "Starting Epoch 5\n",
      "Will sample from train_loader\n",
      "Train: [0/1000 (0%)]\tLoss: 1.256841\tElapsed time: 0:00:00.496181\tFCWeights (Diff, Avg, Total): (2.203427791595459, 0.05029981583356857, 1648.224365234375)\n",
      "Train: [3900/1000 (10%)]\tLoss: 0.922914\tElapsed time: 0:00:43.645203\tFCWeights (Diff, Avg, Total): (1.8215304613113403, 0.05062469094991684, 1658.869873046875)\n",
      "Train: [10600/1000 (20%)]\tLoss: 0.875235\tElapsed time: 0:01:25.147284\tFCWeights (Diff, Avg, Total): (2.721370220184326, 0.051318999379873276, 1681.6209716796875)\n",
      "Train: [14700/1000 (30%)]\tLoss: 0.936858\tElapsed time: 0:02:07.447565\tFCWeights (Diff, Avg, Total): (7.083095550537109, 0.051456909626722336, 1686.1400146484375)\n",
      "Train: [25600/1000 (40%)]\tLoss: 0.919334\tElapsed time: 0:02:49.866020\tFCWeights (Diff, Avg, Total): (3.473111391067505, 0.05214519426226616, 1708.6937255859375)\n",
      "Train: [19500/1000 (50%)]\tLoss: 0.899629\tElapsed time: 0:03:32.889404\tFCWeights (Diff, Avg, Total): (2.3834826946258545, 0.05267108976840973, 1725.92626953125)\n",
      "Train: [38400/1000 (60%)]\tLoss: 0.855223\tElapsed time: 0:04:15.503290\tFCWeights (Diff, Avg, Total): (1.0597164630889893, 0.05290064960718155, 1733.448486328125)\n",
      "Train: [29400/1000 (70%)]\tLoss: 0.924665\tElapsed time: 0:04:58.179647\tFCWeights (Diff, Avg, Total): (3.673574447631836, 0.05407356470823288, 1771.882568359375)\n",
      "Train: [51200/1000 (80%)]\tLoss: 0.970765\tElapsed time: 0:05:39.865580\tFCWeights (Diff, Avg, Total): (2.1213598251342773, 0.054198943078517914, 1775.990966796875)\n",
      "Train: [45000/1000 (90%)]\tLoss: 0.930051\tElapsed time: 0:06:22.449343\tFCWeights (Diff, Avg, Total): (3.801497459411621, 0.054710280150175095, 1792.7464599609375)\n",
      "Epoch: 6/20. Train set: Average loss: 0.9120\tFCWeights (Diff, Avg, Total): (198.01634216308594, 0.05498332530260086, 1801.693603515625)\n",
      "Starting Epoch 6\n",
      "Will sample from train_loader\n",
      "Train: [0/1000 (0%)]\tLoss: 0.623653\tElapsed time: 0:00:00.378222\tFCWeights (Diff, Avg, Total): (3.6304659843444824, 0.05498900264501572, 1801.879638671875)\n",
      "Train: [3900/1000 (10%)]\tLoss: 0.905611\tElapsed time: 0:00:44.024559\tFCWeights (Diff, Avg, Total): (1.439836859703064, 0.055620670318603516, 1822.578125)\n",
      "Train: [7400/1000 (20%)]\tLoss: 0.834372\tElapsed time: 0:01:26.246457\tFCWeights (Diff, Avg, Total): (3.6446938514709473, 0.055789362639188766, 1828.1058349609375)\n",
      "Train: [19200/1000 (30%)]\tLoss: 1.018861\tElapsed time: 0:02:08.013660\tFCWeights (Diff, Avg, Total): (2.6543149948120117, 0.05593564361333847, 1832.899169921875)\n",
      "Train: [16800/1000 (40%)]\tLoss: 0.931987\tElapsed time: 0:02:49.372992\tFCWeights (Diff, Avg, Total): (1.250925898551941, 0.05633173882961273, 1845.87841796875)\n",
      "Train: [24500/1000 (50%)]\tLoss: 0.874513\tElapsed time: 0:03:32.432551\tFCWeights (Diff, Avg, Total): (11.647534370422363, 0.056240715086460114, 1842.895751953125)\n",
      "Train: [30600/1000 (60%)]\tLoss: 0.994425\tElapsed time: 0:04:14.403604\tFCWeights (Diff, Avg, Total): (1.7825965881347656, 0.05682743340730667, 1862.121337890625)\n",
      "Train: [44800/1000 (70%)]\tLoss: 0.877597\tElapsed time: 0:04:56.314390\tFCWeights (Diff, Avg, Total): (2.1866817474365234, 0.056959543377161026, 1866.4503173828125)\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(training_set, batch_size=1, shuffle=False, num_workers=0)\n",
    "val_loader = None  # DataLoader(validation_set, batch_size=1, shuffle=False, num_workers=0)\n",
    "metrics = []  # [EmbeddingL2DistanceMetric(), EmbeddingCosineSimilarityMetric()]\n",
    "fit(train_loader, val_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, start_epoch=start_epoch, save_progress_path=save_path, metrics=metrics, measure_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_state_path = save_path + r\"\\training_state_1.pth\"\n",
    "print(load_state_path)\n",
    "state = torch.load(load_state_path)\n",
    "model.load_state_dict(state['model'])\n",
    "model.eval()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = r'C:\\Users\\root\\Data\\Angiographie'\n",
    "test_set = get_test_set(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance_matrix(embeddings):\n",
    "    distances = []\n",
    "    for i in range(len(embeddings)):\n",
    "        distances_i = []\n",
    "        for j in range(len(embeddings)):\n",
    "            if j < i:\n",
    "                distances_i.append(distances[j][i])\n",
    "            elif j == i:\n",
    "                distances_i.append(0)\n",
    "            else:\n",
    "                val = torch.sum(torch.abs(embeddings[i] - embeddings[j]))\n",
    "                distances_i.append(val.cpu().numpy())\n",
    "        distances.append(distances_i)\n",
    "    distances = np.array(distances)\n",
    "    return distances\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_loader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=1)\n",
    "    for batch_index, sequences in enumerate(test_loader):\n",
    "        # sequences: (batch, video_frame, channel, width, height)\n",
    "        embeddings = []\n",
    "        print (f\"Batch {batch_index + 1}/{len(test_loader)} with {len(sequences[0])} sequences\")\n",
    "        for i in range(len(sequences[0])):\n",
    "            sequence = sequences[:, i]\n",
    "            embedding = model(sequence) # (1, 1000)\n",
    "            embeddings.append(embedding)\n",
    "        distance_matrix = calc_distance_matrix(embeddings)\n",
    "        plt.imshow(distance_matrix)\n",
    "        plt.colorbar()\n",
    "        plt.title(f\"Distance matrix {distance_matrix.shape}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
